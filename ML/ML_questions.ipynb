{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33d32fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97ec21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('instagram_reach.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f9580dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>S.No</th>\n",
       "      <th>USERNAME</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Time since posted</th>\n",
       "      <th>Likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mikequindazzi</td>\n",
       "      <td>Who are #DataScientist and what do they do? &gt;&gt;...</td>\n",
       "      <td>1600</td>\n",
       "      <td>#MachineLearning #AI #DataAnalytics #DataScien...</td>\n",
       "      <td>11</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>drgorillapaints</td>\n",
       "      <td>We all know where it’s going. We just have to ...</td>\n",
       "      <td>880</td>\n",
       "      <td>#deck .#mac #macintosh#sayhello #apple #steve...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>aitrading_official</td>\n",
       "      <td>Alexander Barinov: 4 years as CFO in multinati...</td>\n",
       "      <td>255</td>\n",
       "      <td>#whoiswho #aitrading #ai #aitradingteam#instat...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>opensourcedworkplace</td>\n",
       "      <td>sfad</td>\n",
       "      <td>340</td>\n",
       "      <td>#iot #cre#workplace #CDO #bigdata #technology#...</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>crea.vision</td>\n",
       "      <td>Ever missed a call while your phone was chargi...</td>\n",
       "      <td>304</td>\n",
       "      <td>#instamachinelearning #instabigdata#instamarke...</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  S.No              USERNAME  \\\n",
       "0           0     1         mikequindazzi   \n",
       "1           1     2       drgorillapaints   \n",
       "2           2     3    aitrading_official   \n",
       "3           3     4  opensourcedworkplace   \n",
       "4           4     5           crea.vision   \n",
       "\n",
       "                                             Caption  Followers  \\\n",
       "0  Who are #DataScientist and what do they do? >>...       1600   \n",
       "1  We all know where it’s going. We just have to ...        880   \n",
       "2  Alexander Barinov: 4 years as CFO in multinati...        255   \n",
       "3                                               sfad        340   \n",
       "4  Ever missed a call while your phone was chargi...        304   \n",
       "\n",
       "                                            Hashtags Time since posted  Likes  \n",
       "0  #MachineLearning #AI #DataAnalytics #DataScien...                11    139  \n",
       "1   #deck .#mac #macintosh#sayhello #apple #steve...                 2     23  \n",
       "2  #whoiswho #aitrading #ai #aitradingteam#instat...                 2     25  \n",
       "3  #iot #cre#workplace #CDO #bigdata #technology#...                 3     49  \n",
       "4  #instamachinelearning #instabigdata#instamarke...                 3     30  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47b9d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Time since posted'] = data['Time since posted'].str.replace(' hours','')\n",
    "data['Time since posted'] = data['Time since posted'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04ecc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[['Followers', 'Time since posted']]\n",
    "test_data = data['Likes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b4ae4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train, y_test = train_test_split(train_data, test_data, test_size= 0.3, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae52b5c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19068\\2640855419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5573\u001b[0m         ):\n\u001b[0;32m   5574\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5575\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a817fec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>S.No</th>\n",
       "      <th>USERNAME</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Time since posted</th>\n",
       "      <th>Likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mikequindazzi</td>\n",
       "      <td>Who are #DataScientist and what do they do? &gt;&gt;...</td>\n",
       "      <td>1600</td>\n",
       "      <td>#MachineLearning #AI #DataAnalytics #DataScien...</td>\n",
       "      <td>11</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>drgorillapaints</td>\n",
       "      <td>We all know where it’s going. We just have to ...</td>\n",
       "      <td>880</td>\n",
       "      <td>#deck .#mac #macintosh#sayhello #apple #steve...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>aitrading_official</td>\n",
       "      <td>Alexander Barinov: 4 years as CFO in multinati...</td>\n",
       "      <td>255</td>\n",
       "      <td>#whoiswho #aitrading #ai #aitradingteam#instat...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>opensourcedworkplace</td>\n",
       "      <td>sfad</td>\n",
       "      <td>340</td>\n",
       "      <td>#iot #cre#workplace #CDO #bigdata #technology#...</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>crea.vision</td>\n",
       "      <td>Ever missed a call while your phone was chargi...</td>\n",
       "      <td>304</td>\n",
       "      <td>#instamachinelearning #instabigdata#instamarke...</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>michaelgarza__</td>\n",
       "      <td>328 S. Wetherly Drive, Beverly Hills, CA 90212...</td>\n",
       "      <td>614</td>\n",
       "      <td>#beverlyhills #realestate#losangelesrealestate...</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>dvlp_search</td>\n",
       "      <td>Credit @tristankappel To find more dvlp follow...</td>\n",
       "      <td>450</td>\n",
       "      <td>#workspace #work #developer#development #devel...</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>ecom.space</td>\n",
       "      <td>We are coming up with the Best 21 Books that w...</td>\n",
       "      <td>182</td>\n",
       "      <td>#books #book #motivation #inspiration #life#bo...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>lb3enterprises</td>\n",
       "      <td>We’re only paid to move dirt once. It’s not ju...</td>\n",
       "      <td>2039</td>\n",
       "      <td>#heavyequipment #underconstruction#dozer #real...</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>palmariusdev</td>\n",
       "      <td>Obtén tu tienda en línea ahora.</td>\n",
       "      <td>741</td>\n",
       "      <td>#marketing #programming#development #desarroll...</td>\n",
       "      <td>3</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  S.No              USERNAME  \\\n",
       "0            0     1         mikequindazzi   \n",
       "1            1     2       drgorillapaints   \n",
       "2            2     3    aitrading_official   \n",
       "3            3     4  opensourcedworkplace   \n",
       "4            4     5           crea.vision   \n",
       "..         ...   ...                   ...   \n",
       "95           8    19        michaelgarza__   \n",
       "96           9    21           dvlp_search   \n",
       "97          10    22            ecom.space   \n",
       "98          11    24        lb3enterprises   \n",
       "99          12    25          palmariusdev   \n",
       "\n",
       "                                              Caption  Followers  \\\n",
       "0   Who are #DataScientist and what do they do? >>...       1600   \n",
       "1   We all know where it’s going. We just have to ...        880   \n",
       "2   Alexander Barinov: 4 years as CFO in multinati...        255   \n",
       "3                                                sfad        340   \n",
       "4   Ever missed a call while your phone was chargi...        304   \n",
       "..                                                ...        ...   \n",
       "95  328 S. Wetherly Drive, Beverly Hills, CA 90212...        614   \n",
       "96  Credit @tristankappel To find more dvlp follow...        450   \n",
       "97  We are coming up with the Best 21 Books that w...        182   \n",
       "98  We’re only paid to move dirt once. It’s not ju...       2039   \n",
       "99                    Obtén tu tienda en línea ahora.        741   \n",
       "\n",
       "                                             Hashtags  Time since posted  \\\n",
       "0   #MachineLearning #AI #DataAnalytics #DataScien...                 11   \n",
       "1    #deck .#mac #macintosh#sayhello #apple #steve...                  2   \n",
       "2   #whoiswho #aitrading #ai #aitradingteam#instat...                  2   \n",
       "3   #iot #cre#workplace #CDO #bigdata #technology#...                  3   \n",
       "4   #instamachinelearning #instabigdata#instamarke...                  3   \n",
       "..                                                ...                ...   \n",
       "95  #beverlyhills #realestate#losangelesrealestate...                  3   \n",
       "96  #workspace #work #developer#development #devel...                  3   \n",
       "97  #books #book #motivation #inspiration #life#bo...                  3   \n",
       "98  #heavyequipment #underconstruction#dozer #real...                  3   \n",
       "99  #marketing #programming#development #desarroll...                  3   \n",
       "\n",
       "    Likes  \n",
       "0     139  \n",
       "1      23  \n",
       "2      25  \n",
       "3      49  \n",
       "4      30  \n",
       "..    ...  \n",
       "95     31  \n",
       "96     42  \n",
       "97     10  \n",
       "98    222  \n",
       "99    109  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7486b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes Prediction:\n",
      "Mean Squared Error: 1458.390457111242\n",
      "R-squared Score: -0.028754349863586715\n",
      "Time Since Posted Prediction:\n",
      "Mean Squared Error: 11.890978804055694\n",
      "R-squared Score: -0.0209039539863225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bharg\\AppData\\Local\\Temp\\ipykernel_19068\\1914772817.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['USERNAME'] = label_encoder.fit_transform(X['USERNAME'])\n",
      "C:\\Users\\bharg\\AppData\\Local\\Temp\\ipykernel_19068\\1914772817.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Caption'] = label_encoder.fit_transform(X['Caption'])\n",
      "C:\\Users\\bharg\\AppData\\Local\\Temp\\ipykernel_19068\\1914772817.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Hashtags'] = label_encoder.fit_transform(X['Hashtags'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('instagram_reach.csv')\n",
    "\n",
    "# Select the input features and target variables\n",
    "X = data[['USERNAME', 'Caption', 'Followers', 'Hashtags']]\n",
    "y_likes = data['Likes']\n",
    "y_time_since_posted = data['Time since posted'].str.replace(' hours','').astype(int)\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "X['USERNAME'] = label_encoder.fit_transform(X['USERNAME'])\n",
    "X['Caption'] = label_encoder.fit_transform(X['Caption'])\n",
    "X['Hashtags'] = label_encoder.fit_transform(X['Hashtags'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_likes_train, y_likes_test, y_time_train, y_time_test = train_test_split(X, y_likes, y_time_since_posted, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train[['Followers']] = scaler.fit_transform(X_train[['Followers']])\n",
    "X_test[['Followers']] = scaler.transform(X_test[['Followers']])\n",
    "\n",
    "# Train the model for Likes prediction\n",
    "likes_model = LinearRegression()\n",
    "likes_model.fit(X_train, y_likes_train)\n",
    "y_likes_pred = likes_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Likes prediction model\n",
    "likes_mse = mean_squared_error(y_likes_test, y_likes_pred)\n",
    "likes_r2 = r2_score(y_likes_test, y_likes_pred)\n",
    "print(\"Likes Prediction:\")\n",
    "print(\"Mean Squared Error:\", likes_mse)\n",
    "print(\"R-squared Score:\", likes_r2)\n",
    "\n",
    "# Train the model for Time Since Posted prediction\n",
    "time_model = LinearRegression()\n",
    "time_model.fit(X_train, y_time_train)\n",
    "y_time_pred = time_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Time Since Posted prediction model\n",
    "time_mse = mean_squared_error(y_time_test, y_time_pred)\n",
    "time_r2 = r2_score(y_time_test, y_time_pred)\n",
    "print(\"Time Since Posted Prediction:\")\n",
    "print(\"Mean Squared Error:\", time_mse)\n",
    "print(\"R-squared Score:\", time_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9494ceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bharg\\AppData\\Local\\Temp\\ipykernel_19068\\3612662816.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['USERNAME'] = label_encoder.fit_transform(X['USERNAME'])\n",
      "C:\\Users\\bharg\\AppData\\Local\\Temp\\ipykernel_19068\\3612662816.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Caption'] = label_encoder.fit_transform(X['Caption'])\n",
      "C:\\Users\\bharg\\AppData\\Local\\Temp\\ipykernel_19068\\3612662816.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Hashtags'] = label_encoder.fit_transform(X['Hashtags'])\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 5 Models (Likes Prediction):\n",
      "1. Random Forest\n",
      "   Mean Squared Error: 1119.3361465054872\n",
      "   R-squared Score: 0.21041589098300717\n",
      "\n",
      "2. Linear Regression\n",
      "   Mean Squared Error: 1458.390457111242\n",
      "   R-squared Score: -0.028754349863586715\n",
      "\n",
      "3. Lasso\n",
      "   Mean Squared Error: 1465.6421397932334\n",
      "   R-squared Score: -0.033869715276568346\n",
      "\n",
      "4. ElasticNet\n",
      "   Mean Squared Error: 1521.3365394340062\n",
      "   R-squared Score: -0.07315676327808696\n",
      "\n",
      "5. Logistic Regression\n",
      "   Mean Squared Error: 2155.8\n",
      "   R-squared Score: -0.520709777427427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('instagram_reach.csv')\n",
    "\n",
    "# Select the input features and target variables\n",
    "X = data[['USERNAME', 'Caption', 'Followers', 'Hashtags']]\n",
    "y_likes = data['Likes']\n",
    "y_time_since_posted = data['Time since posted'].str.replace(' hours','').astype(int)\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "X['USERNAME'] = label_encoder.fit_transform(X['USERNAME'])\n",
    "X['Caption'] = label_encoder.fit_transform(X['Caption'])\n",
    "X['Hashtags'] = label_encoder.fit_transform(X['Hashtags'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_likes_train, y_likes_test, y_time_train, y_time_test = train_test_split(X, y_likes, y_time_since_posted, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train[['Followers']] = scaler.fit_transform(X_train[['Followers']])\n",
    "X_test[['Followers']] = scaler.transform(X_test[['Followers']])\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Random Forest': RandomForestRegressor()\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for tuning\n",
    "param_grids = {\n",
    "    'Linear Regression': {},\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "    'ElasticNet': {'alpha': [0.1, 1, 10], 'l1_ratio': [0.2, 0.5, 0.8]},\n",
    "    'Lasso': {'alpha': [0.1, 1, 10]},\n",
    "    'Random Forest': {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}\n",
    "}\n",
    "\n",
    "# Train and evaluate the models\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(X_train, y_likes_train)\n",
    "    y_likes_pred = grid_search.predict(X_test)\n",
    "    mse = mean_squared_error(y_likes_test, y_likes_pred)\n",
    "    r2 = r2_score(y_likes_test, y_likes_pred)\n",
    "    results.append((model_name, mse, r2))\n",
    "\n",
    "# Sort the results based on MSE in ascending order\n",
    "results.sort(key=lambda x: x[1])\n",
    "\n",
    "# Print the results for the best 5 models\n",
    "print(\"Best 5 Models (Likes Prediction):\")\n",
    "for i, (model_name, mse, r2) in enumerate(results[:5], 1):\n",
    "    print(f\"{i}. {model_name}\")\n",
    "    print(\"   Mean Squared Error:\", mse)\n",
    "    print(\"   R-squared Score:\", r2)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9db005d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bharg\\AppData\\Local\\Temp\\ipykernel_19068\\1229909752.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['USERNAME'] = label_encoder.fit_transform(X['USERNAME'])\n",
      "C:\\Users\\bharg\\AppData\\Local\\Temp\\ipykernel_19068\\1229909752.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Caption'] = label_encoder.fit_transform(X['Caption'])\n",
      "C:\\Users\\bharg\\AppData\\Local\\Temp\\ipykernel_19068\\1229909752.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Hashtags'] = label_encoder.fit_transform(X['Hashtags'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 5 Models for Likes Prediction:\n",
      "1. Random Forest\n",
      "   Mean Squared Error: 1273.0188232237724\n",
      "   R-squared Score: 0.10200752791281753\n",
      "\n",
      "2. Linear Regression\n",
      "   Mean Squared Error: 1458.390457111242\n",
      "   R-squared Score: -0.028754349863586715\n",
      "\n",
      "3. Lasso\n",
      "   Mean Squared Error: 1465.6421397932334\n",
      "   R-squared Score: -0.033869715276568346\n",
      "\n",
      "4. ElasticNet\n",
      "   Mean Squared Error: 1521.3365394340062\n",
      "   R-squared Score: -0.07315676327808696\n",
      "\n",
      "Best 5 Models for Time Since Posted Prediction:\n",
      "1. Random Forest\n",
      "   Mean Squared Error: 5.221489865849565\n",
      "   R-squared Score: 0.5517072448293999\n",
      "\n",
      "2. Linear Regression\n",
      "   Mean Squared Error: 11.890978804055694\n",
      "   R-squared Score: -0.0209039539863225\n",
      "\n",
      "3. ElasticNet\n",
      "   Mean Squared Error: 12.022656249999999\n",
      "   R-squared Score: -0.032209165056878764\n",
      "\n",
      "4. Lasso\n",
      "   Mean Squared Error: 12.022656249999999\n",
      "   R-squared Score: -0.032209165056878764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('instagram_reach.csv')\n",
    "\n",
    "# Select the input features and target variables\n",
    "X = data[['USERNAME', 'Caption', 'Followers', 'Hashtags']]\n",
    "y_likes = data['Likes']\n",
    "y_time_since_posted = data['Time since posted'].str.replace(' hours','').astype(int)\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "X['USERNAME'] = label_encoder.fit_transform(X['USERNAME'])\n",
    "X['Caption'] = label_encoder.fit_transform(X['Caption'])\n",
    "X['Hashtags'] = label_encoder.fit_transform(X['Hashtags'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_likes_train, y_likes_test, y_time_train, y_time_test = train_test_split(X, y_likes, y_time_since_posted, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train[['Followers']] = scaler.fit_transform(X_train[['Followers']])\n",
    "X_test[['Followers']] = scaler.transform(X_test[['Followers']])\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Random Forest': RandomForestRegressor()\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for tuning\n",
    "param_grids = {\n",
    "    'Linear Regression': {},\n",
    "    'ElasticNet': {'alpha': [0.1, 1, 10], 'l1_ratio': [0.2, 0.5, 0.8]},\n",
    "    'Lasso': {'alpha': [0.1, 1, 10]},\n",
    "    'Random Forest': {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}\n",
    "}\n",
    "\n",
    "# Train and evaluate the models for Likes prediction\n",
    "likes_results = []\n",
    "for model_name, model in models.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(X_train, y_likes_train)\n",
    "    y_likes_pred = grid_search.predict(X_test)\n",
    "    mse = mean_squared_error(y_likes_test, y_likes_pred)\n",
    "    r2 = r2_score(y_likes_test, y_likes_pred)\n",
    "    likes_results.append((model_name, mse, r2))\n",
    "\n",
    "# Sort the results based on MSE in ascending order for Likes prediction\n",
    "likes_results.sort(key=lambda x: x[1])\n",
    "\n",
    "# Train and evaluate the models for Time Since Posted prediction\n",
    "time_results = []\n",
    "for model_name, model in models.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(X_train, y_time_train)\n",
    "    y_time_pred = grid_search.predict(X_test)\n",
    "    mse = mean_squared_error(y_time_test, y_time_pred)\n",
    "    r2 = r2_score(y_time_test, y_time_pred)\n",
    "    time_results.append((model_name, mse, r2))\n",
    "\n",
    "# Sort the results based on MSE in ascending order for Time Since Posted prediction\n",
    "time_results.sort(key=lambda x: x[1])\n",
    "\n",
    "# Print the results for the best 5 models for Likes prediction\n",
    "print(\"Best 5 Models for Likes Prediction:\")\n",
    "for i, (model_name, mse, r2) in enumerate(likes_results[:5], 1):\n",
    "    print(f\"{i}. {model_name}\")\n",
    "    print(\"   Mean Squared Error:\", mse)\n",
    "    print(\"   R-squared Score:\", r2)\n",
    "    print()\n",
    "\n",
    "# Print the results for the best 5 models for Time Since Posted prediction\n",
    "print(\"Best 5 Models for Time Since Posted Prediction:\")\n",
    "for i, (model_name, mse, r2) in enumerate(time_results[:5], 1):\n",
    "    print(f\"{i}. {model_name}\")\n",
    "    print(\"   Mean Squared Error:\", mse)\n",
    "    print(\"   R-squared Score:\", r2)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314788a4",
   "metadata": {},
   "source": [
    "# Q.2.\n",
    "Imagine you have a dataset where you have different features like Age ,\n",
    "Gender , Height , Weight , BMI , and Blood Pressure and you have to classify the people into\n",
    "different classes like Normal , Overweight , Obesity , Underweight , and Extreme Obesity by using\n",
    "any 4 different classification algorithms. Now you have to build a model which\n",
    "can classify people into different classes.\n",
    "\n",
    "https://www.kaggle.com/datasets/ankurbajaj9/obesity-levels\n",
    "This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0506b2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bharg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.6548463356973995\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.74      0.93      0.83        56\n",
      "      Normal_Weight       0.53      0.42      0.47        62\n",
      "     Obesity_Type_I       0.58      0.60      0.59        78\n",
      "    Obesity_Type_II       0.82      0.84      0.83        58\n",
      "   Obesity_Type_III       0.90      1.00      0.95        63\n",
      " Overweight_Level_I       0.54      0.38      0.44        56\n",
      "Overweight_Level_II       0.35      0.38      0.37        50\n",
      "\n",
      "           accuracy                           0.65       423\n",
      "          macro avg       0.64      0.65      0.64       423\n",
      "       weighted avg       0.64      0.65      0.64       423\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.9385342789598109\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.93      0.96      0.95        56\n",
      "      Normal_Weight       0.89      0.90      0.90        62\n",
      "     Obesity_Type_I       0.95      0.92      0.94        78\n",
      "    Obesity_Type_II       0.93      0.95      0.94        58\n",
      "   Obesity_Type_III       1.00      1.00      1.00        63\n",
      " Overweight_Level_I       0.90      0.93      0.91        56\n",
      "Overweight_Level_II       0.98      0.90      0.94        50\n",
      "\n",
      "           accuracy                           0.94       423\n",
      "          macro avg       0.94      0.94      0.94       423\n",
      "       weighted avg       0.94      0.94      0.94       423\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.9550827423167849\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.98      0.96      0.97        56\n",
      "      Normal_Weight       0.89      0.90      0.90        62\n",
      "     Obesity_Type_I       0.99      0.97      0.98        78\n",
      "    Obesity_Type_II       0.97      0.98      0.97        58\n",
      "   Obesity_Type_III       1.00      1.00      1.00        63\n",
      " Overweight_Level_I       0.88      0.89      0.88        56\n",
      "Overweight_Level_II       0.98      0.96      0.97        50\n",
      "\n",
      "           accuracy                           0.96       423\n",
      "          macro avg       0.95      0.95      0.95       423\n",
      "       weighted avg       0.96      0.96      0.96       423\n",
      "\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.5650118203309693\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.71      0.88      0.78        56\n",
      "      Normal_Weight       0.48      0.34      0.40        62\n",
      "     Obesity_Type_I       0.65      0.33      0.44        78\n",
      "    Obesity_Type_II       0.77      0.41      0.54        58\n",
      "   Obesity_Type_III       0.56      1.00      0.72        63\n",
      " Overweight_Level_I       0.47      0.48      0.47        56\n",
      "Overweight_Level_II       0.43      0.58      0.49        50\n",
      "\n",
      "           accuracy                           0.57       423\n",
      "          macro avg       0.58      0.57      0.55       423\n",
      "       weighted avg       0.59      0.57      0.54       423\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data, meta = arff.loadarff('ObesityDataSet_raw_and_data_sinthetic.arff')\n",
    "data = pd.DataFrame(data)\n",
    "data = data.applymap(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "\n",
    "# Extract the feature matrix X and the target variable y\n",
    "X = data.drop('NObeyesdad', axis=1)\n",
    "y = data['NObeyesdad']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns \n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train the classification models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC()\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49b23311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.8747044917257684\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.84      1.00      0.91        56\n",
      "      Normal_Weight       0.90      0.61      0.73        62\n",
      "     Obesity_Type_I       0.95      0.90      0.92        78\n",
      "    Obesity_Type_II       0.89      0.97      0.93        58\n",
      "   Obesity_Type_III       1.00      1.00      1.00        63\n",
      " Overweight_Level_I       0.76      0.79      0.77        56\n",
      "Overweight_Level_II       0.77      0.86      0.81        50\n",
      "\n",
      "           accuracy                           0.87       423\n",
      "          macro avg       0.87      0.87      0.87       423\n",
      "       weighted avg       0.88      0.87      0.87       423\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.9408983451536643\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.92      0.96      0.94        56\n",
      "      Normal_Weight       0.87      0.89      0.88        62\n",
      "     Obesity_Type_I       0.96      0.92      0.94        78\n",
      "    Obesity_Type_II       0.93      0.95      0.94        58\n",
      "   Obesity_Type_III       1.00      1.00      1.00        63\n",
      " Overweight_Level_I       0.93      0.91      0.92        56\n",
      "Overweight_Level_II       0.98      0.96      0.97        50\n",
      "\n",
      "           accuracy                           0.94       423\n",
      "          macro avg       0.94      0.94      0.94       423\n",
      "       weighted avg       0.94      0.94      0.94       423\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.9598108747044918\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       1.00      0.96      0.98        56\n",
      "      Normal_Weight       0.89      0.94      0.91        62\n",
      "     Obesity_Type_I       0.99      0.96      0.97        78\n",
      "    Obesity_Type_II       0.97      0.98      0.97        58\n",
      "   Obesity_Type_III       1.00      1.00      1.00        63\n",
      " Overweight_Level_I       0.89      0.89      0.89        56\n",
      "Overweight_Level_II       0.98      0.98      0.98        50\n",
      "\n",
      "           accuracy                           0.96       423\n",
      "          macro avg       0.96      0.96      0.96       423\n",
      "       weighted avg       0.96      0.96      0.96       423\n",
      "\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.91725768321513\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.93      0.98      0.96        56\n",
      "      Normal_Weight       0.86      0.81      0.83        62\n",
      "     Obesity_Type_I       0.96      0.94      0.95        78\n",
      "    Obesity_Type_II       0.92      0.98      0.95        58\n",
      "   Obesity_Type_III       1.00      1.00      1.00        63\n",
      " Overweight_Level_I       0.80      0.86      0.83        56\n",
      "Overweight_Level_II       0.93      0.84      0.88        50\n",
      "\n",
      "           accuracy                           0.92       423\n",
      "          macro avg       0.92      0.91      0.91       423\n",
      "       weighted avg       0.92      0.92      0.92       423\n",
      "\n",
      "\n",
      "Best Model: Random Forest\n",
      "Best Model Classification Report:\n",
      "{'Insufficient_Weight': {'precision': 1.0, 'recall': 0.9642857142857143, 'f1-score': 0.9818181818181818, 'support': 56}, 'Normal_Weight': {'precision': 0.8923076923076924, 'recall': 0.9354838709677419, 'f1-score': 0.9133858267716536, 'support': 62}, 'Obesity_Type_I': {'precision': 0.9868421052631579, 'recall': 0.9615384615384616, 'f1-score': 0.974025974025974, 'support': 78}, 'Obesity_Type_II': {'precision': 0.9661016949152542, 'recall': 0.9827586206896551, 'f1-score': 0.9743589743589743, 'support': 58}, 'Obesity_Type_III': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 63}, 'Overweight_Level_I': {'precision': 0.8928571428571429, 'recall': 0.8928571428571429, 'f1-score': 0.8928571428571429, 'support': 56}, 'Overweight_Level_II': {'precision': 0.98, 'recall': 0.98, 'f1-score': 0.98, 'support': 50}, 'accuracy': 0.9598108747044918, 'macro avg': {'precision': 0.9597298050490354, 'recall': 0.9595605443341023, 'f1-score': 0.9594922999759896, 'support': 423}, 'weighted avg': {'precision': 0.9605925755051725, 'recall': 0.9598108747044918, 'f1-score': 0.9600439383652652, 'support': 423}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data, meta = arff.loadarff('ObesityDataSet_raw_and_data_sinthetic.arff')\n",
    "data = pd.DataFrame(data)\n",
    "data = data.applymap(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "# Extract the feature matrix X and the target variable y\n",
    "X = data.drop('NObeyesdad', axis=1)\n",
    "y = data['NObeyesdad']\n",
    "\n",
    "# Custom transformer for label encoding\n",
    "class LabelEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in X.columns:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(X[col])\n",
    "            self.label_encoders[col] = le\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_encoded = X.copy()\n",
    "        for col, le in self.label_encoders.items():\n",
    "            X_encoded[col] = le.transform(X[col])\n",
    "        return X_encoded\n",
    "\n",
    "# Define the column transformer for encoding categorical variables and scaling numerical variables\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numerical_cols = X.select_dtypes(include=['float', 'int']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('label_encoder', LabelEncoderTransformer(), categorical_cols.tolist()),\n",
    "        ('scaler', StandardScaler(), numerical_cols.tolist())\n",
    "    ])\n",
    "\n",
    "# Define the classification models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC()\n",
    "}\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    # Create the pipeline with preprocessing and the classification model\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Split the dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    results[model_name] = classification_report(y_test, y_pred,output_dict=True)\n",
    "    # Print the results\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print()\n",
    "\n",
    "best_model = max(results, key=lambda x: results[x]['accuracy'])\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(\"Best Model Classification Report:\")\n",
    "print(results[best_model])\n",
    "\n",
    "\n",
    "#creating the best model\n",
    "pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', {best_model})\n",
    "    ])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuaracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e18c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
